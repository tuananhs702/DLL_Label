{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict\n",
    "from underthesea import word_tokenize\n",
    "from xgboost import XGBClassifier\n",
    "import random\n",
    "import re\n",
    "from underthesea import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "\n",
    "\n",
    "df_dl=pd.read_csv('df_dl_test.csv')\n",
    "df_result=df_dl[['Product List','label']]\n",
    "df_result=pd.DataFrame(df_result)\n",
    "df_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize text\n",
    "vocab_size = 30000\n",
    "window_size = 2  # Number of words before & after target word\n",
    "embedding_dim = 150\n",
    "df_result[\"Tokenized\"] = df_result[\"Product List\"].apply(lambda x: word_tokenize(x, format=\"text\").split())\n",
    "\n",
    "# Build vocabulary\n",
    "word_counts = defaultdict(int)\n",
    "for tokens in df_result[\"Tokenized\"]:\n",
    "    for word in tokens:\n",
    "        word_counts[word] += 1\n",
    "\n",
    "# Assign indices to words\n",
    "vocab = list(word_counts.keys())[:vocab_size]\n",
    "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
    "\n",
    "# Initialize co-occurrence matrix\n",
    "co_occurrence_matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "# Populate the co-occurrence matrix\n",
    "for tokens in df_result[\"Tokenized\"]:\n",
    "    token_indices = [word_to_index[word] for word in tokens if word in word_to_index]\n",
    "    for idx, word_idx in enumerate(token_indices):\n",
    "        left_context = token_indices[max(0, idx - window_size): idx]\n",
    "        right_context = token_indices[idx + 1: idx + 1 + window_size]\n",
    "        context = left_context + right_context\n",
    "        for context_idx in context:\n",
    "            co_occurrence_matrix[word_idx, context_idx] += 1  # Count occurrences\n",
    "\n",
    "co_occurrence_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SGD ,mini batch\n",
    "import numpy as np\n",
    "\n",
    "# Define the weighting function\n",
    "X_max = 100  # Threshold\n",
    "alpha = 0.75\n",
    "\n",
    "def f(X_ij):\n",
    "    return (X_ij / (X_max + 1e-8)) ** alpha if X_ij < X_max else 1\n",
    "\n",
    "# Hyperparameters\n",
    "learning_rate = 0.01  # Start with a higher LR, then decay\n",
    "num_epochs = 50\n",
    "batch_size = 500  # Mini-batch size\n",
    "decay_factor = 0.99  # Learning rate decay\n",
    "clip_value = 5  # Gradient clipping\n",
    "\n",
    "# Initialize embeddings\n",
    "vocab_size = len(vocab)\n",
    "word_embeddings = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim))\n",
    "context_embeddings = np.random.uniform(-0.5, 0.5, (vocab_size, embedding_dim))\n",
    "\n",
    "nonzero_pairs = np.array([(int(i), int(j), float(co_occurrence_matrix[i, j]))\n",
    "                          for i in range(vocab_size)\n",
    "                          for j in range(vocab_size) if co_occurrence_matrix[i, j] > 0], dtype=object)\n",
    "\n",
    "\n",
    "# Training loop with SGD\n",
    "for epoch in range(num_epochs):\n",
    "    np.random.shuffle(nonzero_pairs)  # Shuffle dataset each epoch\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch_start in range(0, len(nonzero_pairs), batch_size):\n",
    "        batch = nonzero_pairs[batch_start: batch_start + batch_size]\n",
    "\n",
    "        for i, j, X_ij in batch:\n",
    "            weight = f(X_ij)\n",
    "            dot_product = np.dot(word_embeddings[i], context_embeddings[j])\n",
    "\n",
    "            # Loss function\n",
    "            loss = weight * ((dot_product - np.log(X_ij + 1e-8)) ** 2)\n",
    "            total_loss += loss\n",
    "\n",
    "            # Compute gradients\n",
    "            grad_u = 2 * weight * (dot_product - np.log(X_ij + 1e-8)) * context_embeddings[j]\n",
    "            grad_v = 2 * weight * (dot_product - np.log(X_ij + 1e-8)) * word_embeddings[i]\n",
    "\n",
    "            # Gradient clipping\n",
    "            grad_u = np.clip(grad_u, -clip_value, clip_value)\n",
    "            grad_v = np.clip(grad_v, -clip_value, clip_value)\n",
    "\n",
    "            # SGD Updates\n",
    "            word_embeddings[i] -= learning_rate * grad_u\n",
    "            context_embeddings[j] -= learning_rate * grad_v\n",
    "\n",
    "    # Decay learning rate\n",
    "    learning_rate *= decay_factor\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, Loss: {total_loss:.4f}, Learning Rate: {learning_rate:.6f}\")\n",
    "\n",
    "# Merge embeddings\n",
    "final_embeddings = word_embeddings + context_embeddings\n",
    "\n",
    "# Normalize embeddings\n",
    "final_embeddings /= (np.linalg.norm(final_embeddings, axis=1, keepdims=True) + 1e-8)\n",
    "\n",
    "# Create word embedding dictionary\n",
    "word_embedding_dict = {word: final_embeddings[i] for word, i in word_to_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply embeddings to dataset\n",
    "one_hot_encoder = OneHotEncoder(sparse_output=False)\n",
    "df_result[\"Encoded_Label\"] = list(one_hot_encoder.fit_transform(df_result[\"label\"].values.reshape(-1, 1)))\n",
    "def sentence_to_vector(sentence, embedding_dict):\n",
    "    word_vectors = [embedding_dict[word] for word in sentence if word in embedding_dict]\n",
    "    return np.mean(word_vectors, axis=0) if word_vectors else np.zeros(embedding_dim)\n",
    "\n",
    "df_result[\"Embedding\"] = df_result[\"Tokenized\"].apply(lambda x: sentence_to_vector(x, word_embedding_dict))\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, train_indices, test_indices = train_test_split(\n",
    "    np.vstack(df_result[\"Embedding\"].values), \n",
    "    np.vstack(df_result[\"Encoded_Label\"].values), \n",
    "    df_result.index,  # Track original indices\n",
    "    test_size=0.2, \n",
    "    shuffle=True\n",
    ") \n",
    "\n",
    "# Store test indices\n",
    "X_test_indices = test_indices\n",
    "\n",
    "xgb_classifier = XGBClassifier(\n",
    "    use_label_encoder=False,\n",
    "    eval_metric='logloss',\n",
    "    objective=\"binary:logistic\",  # Suitable for multi-label tasks\n",
    "    booster='gbtree',  # Better decision-making\n",
    "    n_estimators=500,  # Increased number of boosting rounds\n",
    "    learning_rate=0.05,  # Slower learning for better generalization\n",
    "    max_depth=6,  # Increase tree depth\n",
    "    subsample=0.8,  # Reduce overfitting\n",
    "    colsample_bytree=0.8  # Feature sampling\n",
    ")\n",
    "classifier = MultiOutputClassifier(xgb_classifier)\n",
    "classifier.fit(X_train, y_train)\n",
    "# Train a multi-label classifier\n",
    "#classifier = MultiOutputClassifier(LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=10000))\n",
    "                                                      \n",
    "#classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred_proba = np.array([clf.predict_proba(X_test)[:, 1] for clf in classifier.estimators_]).T  # Fix Shape Issue\n",
    "threshold = 0.5\n",
    "y_pred = (y_pred_proba >= threshold).astype(int)  # Apply threshold\n",
    "print(\"y_test shape:\", y_test.shape)  \n",
    "print(\"y_pred shape:\", y_pred.shape)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Classification Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert predicted one-hot encoded labels back to original label names\n",
    "y_pred_labels = []\n",
    "for row in y_pred:\n",
    "    if row.sum() == 0:  # If all values are zero, assign \"Unknown\"\n",
    "        y_pred_labels.append([\"Unknown\"])\n",
    "    else:\n",
    "        y_pred_labels.append(one_hot_encoder.inverse_transform(row.reshape(1, -1))[0])\n",
    "\n",
    "# Convert to a NumPy array\n",
    "y_pred_labels = np.array(y_pred_labels).flatten()\n",
    "y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test = df_result.loc[X_test_indices, ['Product List']].copy()\n",
    "df_X_test['predict_label'] = y_pred_labels  # Assign predicted labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_X_test"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
